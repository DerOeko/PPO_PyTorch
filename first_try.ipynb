{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Package downloads\n",
    "!pip install gymnasium --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Package imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import numpy as np\n",
    "import random\n",
    "import typing\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Helper functions\n",
    "class DiscretePolicyNetwork(nn.Module):\n",
    "  def __init__(self, num_cells, action_space_size):\n",
    "    super(DiscretePolicyNetwork, self).__init__()\n",
    "    self.action_space_size = action_space_size\n",
    "    self.dense1 = nn.LazyLinear(num_cells)\n",
    "    self.dense2 = nn.LazyLinear(num_cells)\n",
    "    self.dense3 = nn.LazyLinear(num_cells)\n",
    "    self.output = nn.LazyLinear(action_space_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.leaky_relu(self.dense1(x), negative_slope=0.01)\n",
    "    x = F.leaky_relu(self.dense2(x), negative_slope=0.01)\n",
    "    x = F.leaky_relu(self.dense3(x), negative_slope=0.01)\n",
    "    x = F.softmax(self.output(x), dim=-1)\n",
    "    return x\n",
    "\n",
    "  def generate_actions(self, x):\n",
    "    phi_ts = self(x)\n",
    "    m = torch.distributions.Categorical(phi_ts)\n",
    "    actions = m.sample()\n",
    "    action_probs = torch.gather(phi_ts, 1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "    return actions, action_probs, phi_ts\n",
    "\n",
    "  def generate_action(self, x):\n",
    "    phi_t = self(x).squeeze()\n",
    "    action = torch.distributions.Categorical(phi_t).sample()\n",
    "    action_prob = phi_t[action]\n",
    "    return action, action_prob, phi_t\n",
    "\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "  def __init__(self, num_cells):\n",
    "    super(ValueNetwork, self).__init__()\n",
    "    self.dense1 = nn.LazyLinear(num_cells)\n",
    "    self.dense2 = nn.LazyLinear(num_cells)\n",
    "    self.dense3 = nn.LazyLinear(num_cells)\n",
    "    self.output = nn.LazyLinear(1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.leaky_relu(self.dense1(x), negative_slope=0.01)\n",
    "    x = F.leaky_relu(self.dense2(x), negative_slope=0.01)\n",
    "    x = F.leaky_relu(self.dense3(x), negative_slope=0.01)\n",
    "    x = self.output(x)\n",
    "    return x\n",
    "\n",
    "# helper function for creating minibatches\n",
    "def create_minibatches(data, batch_size):\n",
    "  random.shuffle(data)\n",
    "  minibatches = []\n",
    "\n",
    "  for i in range(0, len(data), batch_size):\n",
    "    mini_batch = data[i:i + batch_size]\n",
    "    minibatches.append(mini_batch)\n",
    "\n",
    "  return minibatches\n",
    "\n",
    "def collect_trajectory(T, state, policy_net, angle_penalty):\n",
    "  trajectory = []\n",
    "  done = False\n",
    "\n",
    "  for j in range(T):\n",
    "    # Check if the trajectory has ended and break the loop if it has\n",
    "      if done:\n",
    "          break\n",
    "\n",
    "      # Generate action and action probability from the policy network\n",
    "      action, action_prob, _ = policy_net.generate_action(state)\n",
    "\n",
    "      # Take a step in the environment using the chosen action\n",
    "      next_state, reward, done, _, _ = env.step(action.item())\n",
    "      angle = next_state[2]\n",
    "\n",
    "      shaped_reward = reward - abs(angle) * angle_penalty\n",
    "      # Append the state, action, reward, and action probability to the trajectory\n",
    "      trajectory.append([state, action, shaped_reward, action_prob])\n",
    "\n",
    "      state = torch.tensor(next_state)\n",
    "\n",
    "  return trajectory\n",
    "\n",
    "def extend_trajectory(T, trajectory, value_net, gamma, lambda_):\n",
    "    states = torch.stack([observation[0] for observation in trajectory])\n",
    "    state_values = value_net(states).squeeze().detach().numpy()\n",
    "\n",
    "    rewards = [observation[2] for observation in trajectory]\n",
    "    dones = [1.0 if t == len(trajectory) - 1 else 0.0 for t in range(len(trajectory))]\n",
    "\n",
    "    advantages = []\n",
    "    gae = 0\n",
    "    for t in reversed(range(len(trajectory))):\n",
    "        if t == len(trajectory) - 1:\n",
    "            delta = rewards[t] - state_values[t]\n",
    "        else:\n",
    "            delta = rewards[t] + gamma * state_values[t + 1] * (1 - dones[t]) - state_values[t]\n",
    "        gae = delta + gamma * lambda_ * (1 - dones[t]) * gae\n",
    "        advantages.append(gae)\n",
    "\n",
    "    advantages.reverse()\n",
    "    advantages = torch.tensor(advantages, dtype=torch.float32)\n",
    "\n",
    "    returns = []\n",
    "    for t in range(len(trajectory)):\n",
    "        if t == len(trajectory) - 1:\n",
    "            returns.append(advantages[t])\n",
    "        else:\n",
    "            returns.append(advantages[t] + state_values[t])\n",
    "\n",
    "    for t in range(len(trajectory)):\n",
    "        trajectory[t].extend([advantages[t], returns[t]])\n",
    "\n",
    "    return trajectory\n",
    "\"\"\"\n",
    "def extend_trajectory(T, trajectory, value_net):\n",
    "  states = torch.stack([observation[0] for observation in trajectory])\n",
    "  state_values = value_net(states).squeeze()\n",
    "  next_state_value = state_values[-1] if (len(trajectory) == T) else 0\n",
    "\n",
    "  for t in range(len(trajectory) - 1, -1, -1):\n",
    "\n",
    "      reward = trajectory[t][2]\n",
    "\n",
    "      if t == len(trajectory) - 1:\n",
    "          # For the last step, calculate V_target using the next state value\n",
    "          V_target = reward + discount_rate * next_state_value\n",
    "      else:\n",
    "          # For other steps, calculate V_target using the V_target of the next step\n",
    "          V_target = reward + discount_rate * trajectory[t + 1][-1]\n",
    "\n",
    "      # Get the value network's estimate for the current state and calculate the advantage\n",
    "      A = V_target - state_values[t]\n",
    "\n",
    "      # Extend the step data with the calculated advantage and V_target\n",
    "      trajectory[t].extend([A, V_target])\n",
    "\n",
    "  return trajectory\n",
    "\"\"\"\n",
    "def calc_lclipvh(minibatch, policy_network, value_network, v, h, clip_epsilon):\n",
    "    policies_new = []\n",
    "    policies_old = []\n",
    "    advantages = []\n",
    "    states = []\n",
    "    V_targets = []\n",
    "    phi_ts = []\n",
    "\n",
    "    # Collect data for each step in the trajectory\n",
    "    for trajectory in minibatch:\n",
    "        for step in trajectory:\n",
    "            state, action, reward, policy_old, advantage, v_target = step\n",
    "            policies_old.append(policy_old)\n",
    "            advantages.append(advantage)\n",
    "            states.append(torch.tensor(state, dtype=torch.float32))  # Convert to PyTorch tensor\n",
    "            V_targets.append(v_target)\n",
    "\n",
    "    # Convert lists to PyTorch tensors\n",
    "    policies_old = torch.tensor(policies_old, dtype=torch.float32)\n",
    "    advantages = torch.tensor(advantages, dtype=torch.float32)\n",
    "    V_targets = torch.tensor(V_targets, dtype=torch.float32)\n",
    "\n",
    "    L_clip, phi_ts = calc_lclip(policy_network, states, advantages, policies_old, clip_epsilon)\n",
    "    print(f\"L_clip before negation: {L_clip}\")\n",
    "    print(f\"Expected sign: positive before negation, as it's a mean of positive values\")\n",
    "    L_v = calc_lv(states, V_targets, value_network)\n",
    "    H = calc_h(phi_ts)\n",
    "    print(f\"Final loss: {-L_clip + v * L_v - h * H}\")\n",
    "    print(f\"Expected sign: could be positive or negative depending on the balance of the components\")\n",
    "\n",
    "    return L_v, -L_clip + v * L_v - h * H\n",
    "\n",
    "def calc_lv(states, V_targets, value_network):\n",
    "    state_values = value_network(torch.stack(states)).flatten()  # Ensure states is a tensor\n",
    "    squared_differences = (state_values - V_targets)**2\n",
    "    return torch.mean(squared_differences)\n",
    "\n",
    "def calc_h(phi_ts):\n",
    "    entropy_terms = phi_ts * torch.log(phi_ts)\n",
    "    print(f\"Phi_ts: {phi_ts}\")\n",
    "    print(f\"Entropy terms: {entropy_terms}\")\n",
    "    print(f\"Expected: phi_ts should be a probability distribution; entropy terms should be negative\")\n",
    "\n",
    "    return -torch.mean(entropy_terms)\n",
    "\n",
    "def calc_lclip(policy_network, states, advantages, policies_old, clip_epsilon):\n",
    "    states_tensor = torch.stack(states)  # Ensure states is a tensor\n",
    "    _, policies_new, phi_ts = policy_network.generate_actions(states_tensor)\n",
    "    policy_ratios = policies_new / policies_old\n",
    "    clipped_ratios = torch.clamp(policy_ratios, 1 - clip_epsilon, 1 + clip_epsilon)\n",
    "    clipped_objectives = torch.min(policy_ratios * advantages, clipped_ratios * advantages)\n",
    "    print(f\"Policy ratios: {policy_ratios}\")\n",
    "    print(f\"Expected range: close to 1, within [{1 - clip_epsilon}, {1 + clip_epsilon}] for non-clipped values\")\n",
    "    return torch.mean(clipped_objectives), phi_ts\n",
    "\n",
    "def init_weights(m):\n",
    "  if type(m) == nn.LazyLinear:\n",
    "    init.orthogonal_(m.weight)\n",
    "    m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 149\u001b[0m\n\u001b[1;32m    146\u001b[0m     plt\u001b[38;5;241m.\u001b[39mlegend()\n\u001b[1;32m    148\u001b[0m     plt\u001b[38;5;241m.\u001b[39mtight_layout()\n\u001b[0;32m--> 149\u001b[0m     \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m     outer_loop \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# Turn off interactive plotting for final display\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/PPOenv/lib/python3.12/site-packages/matplotlib/pyplot.py:527\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;124;03mDisplay all open figures.\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;124;03mexplicitly there.\u001b[39;00m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    526\u001b[0m _warn_if_gui_out_of_main_thread()\n\u001b[0;32m--> 527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_backend_mod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/PPOenv/lib/python3.12/site-packages/matplotlib_inline/backend_inline.py:90\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m figure_manager \u001b[38;5;129;01min\u001b[39;00m Gcf\u001b[38;5;241m.\u001b[39mget_all_fig_managers():\n\u001b[0;32m---> 90\u001b[0m         \u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fetch_figure_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     show\u001b[38;5;241m.\u001b[39m_to_draw \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/mambaforge/envs/PPOenv/lib/python3.12/site-packages/IPython/core/display_functions.py:298\u001b[0m, in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m     publish_display_data(data\u001b[38;5;241m=\u001b[39mobj, metadata\u001b[38;5;241m=\u001b[39mmetadata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 298\u001b[0m     format_dict, md_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m format_dict:\n\u001b[1;32m    300\u001b[0m         \u001b[38;5;66;03m# nothing to display (e.g. _ipython_display_ took over)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/PPOenv/lib/python3.12/site-packages/IPython/core/formatters.py:182\u001b[0m, in \u001b[0;36mDisplayFormatter.format\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    180\u001b[0m md \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 182\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# FIXME: log the exception\u001b[39;00m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/PPOenv/lib/python3.12/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/PPOenv/lib/python3.12/site-packages/IPython/core/formatters.py:226\u001b[0m, in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 226\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_return(\u001b[38;5;28;01mNone\u001b[39;00m, args[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/mambaforge/envs/PPOenv/lib/python3.12/site-packages/IPython/core/formatters.py:343\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[1;32m    345\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n",
      "File \u001b[0;32m~/mambaforge/envs/PPOenv/lib/python3.12/site-packages/IPython/core/pylabtools.py:152\u001b[0m, in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[1;32m    150\u001b[0m     FigureCanvasBase(fig)\n\u001b[0;32m--> 152\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbytes_io\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m data \u001b[38;5;241m=\u001b[39m bytes_io\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fmt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/mambaforge/envs/PPOenv/lib/python3.12/site-packages/matplotlib/backend_bases.py:2167\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bbox_inches:\n\u001b[1;32m   2166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 2167\u001b[0m         bbox_inches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tightbbox\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2168\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox_extra_artists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbox_extra_artists\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2169\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(layout_engine, ConstrainedLayoutEngine) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m   2170\u001b[0m                 pad_inches \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   2171\u001b[0m             h_pad \u001b[38;5;241m=\u001b[39m layout_engine\u001b[38;5;241m.\u001b[39mget()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh_pad\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/mambaforge/envs/PPOenv/lib/python3.12/site-packages/matplotlib/_api/deprecation.py:454\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m name_idx:\n\u001b[1;32m    449\u001b[0m     warn_deprecated(\n\u001b[1;32m    450\u001b[0m         since, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing the \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%(obj_type)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositionally is deprecated since Matplotlib \u001b[39m\u001b[38;5;132;01m%(since)s\u001b[39;00m\u001b[38;5;124m; the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter will become keyword-only \u001b[39m\u001b[38;5;132;01m%(removal)s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    453\u001b[0m         name\u001b[38;5;241m=\u001b[39mname, obj_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/PPOenv/lib/python3.12/site-packages/matplotlib/figure.py:1783\u001b[0m, in \u001b[0;36mFigureBase.get_tightbbox\u001b[0;34m(self, renderer, bbox_extra_artists)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ax\u001b[38;5;241m.\u001b[39mget_visible():\n\u001b[1;32m   1780\u001b[0m     \u001b[38;5;66;03m# some axes don't take the bbox_extra_artists kwarg so we\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m     \u001b[38;5;66;03m# need this conditional....\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1783\u001b[0m         bbox \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tightbbox\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox_extra_artists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbox_extra_artists\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1785\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   1786\u001b[0m         bbox \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39mget_tightbbox(renderer)\n",
      "File \u001b[0;32m~/mambaforge/envs/PPOenv/lib/python3.12/site-packages/matplotlib/_api/deprecation.py:454\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m name_idx:\n\u001b[1;32m    449\u001b[0m     warn_deprecated(\n\u001b[1;32m    450\u001b[0m         since, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing the \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%(obj_type)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositionally is deprecated since Matplotlib \u001b[39m\u001b[38;5;132;01m%(since)s\u001b[39;00m\u001b[38;5;124m; the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter will become keyword-only \u001b[39m\u001b[38;5;132;01m%(removal)s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    453\u001b[0m         name\u001b[38;5;241m=\u001b[39mname, obj_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/PPOenv/lib/python3.12/site-packages/matplotlib/axes/_base.py:4398\u001b[0m, in \u001b[0;36m_AxesBase.get_tightbbox\u001b[0;34m(self, renderer, call_axes_locator, bbox_extra_artists, for_layout_only)\u001b[0m\n\u001b[1;32m   4396\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m ba:\n\u001b[1;32m   4397\u001b[0m             bb\u001b[38;5;241m.\u001b[39mappend(ba)\n\u001b[0;32m-> 4398\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_title_position\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4399\u001b[0m axbbox \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_window_extent(renderer)\n\u001b[1;32m   4400\u001b[0m bb\u001b[38;5;241m.\u001b[39mappend(axbbox)\n",
      "File \u001b[0;32m~/mambaforge/envs/PPOenv/lib/python3.12/site-packages/matplotlib/axes/_base.py:2978\u001b[0m, in \u001b[0;36m_AxesBase._update_title_position\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2976\u001b[0m top \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(top, bb\u001b[38;5;241m.\u001b[39mymax)\n\u001b[1;32m   2977\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m title\u001b[38;5;241m.\u001b[39mget_text():\n\u001b[0;32m-> 2978\u001b[0m     \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43myaxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tightbbox\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# update offsetText\u001b[39;00m\n\u001b[1;32m   2979\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ax\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39moffsetText\u001b[38;5;241m.\u001b[39mget_text():\n\u001b[1;32m   2980\u001b[0m         bb \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39moffsetText\u001b[38;5;241m.\u001b[39mget_tightbbox(renderer)\n",
      "File \u001b[0;32m~/mambaforge/envs/PPOenv/lib/python3.12/site-packages/matplotlib/axis.py:1336\u001b[0m, in \u001b[0;36mAxis.get_tightbbox\u001b[0;34m(self, renderer, for_layout_only)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     renderer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39m_get_renderer()\n\u001b[1;32m   1334\u001b[0m ticks_to_draw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_ticks()\n\u001b[0;32m-> 1336\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_label_position\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1338\u001b[0m \u001b[38;5;66;03m# go back to just this axis's tick labels\u001b[39;00m\n\u001b[1;32m   1339\u001b[0m tlb1, tlb2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ticklabel_bboxes(ticks_to_draw, renderer)\n",
      "File \u001b[0;32m~/mambaforge/envs/PPOenv/lib/python3.12/site-packages/matplotlib/axis.py:2614\u001b[0m, in \u001b[0;36mYAxis._update_label_position\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2612\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2613\u001b[0m     spine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39mspines[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m-> 2614\u001b[0m     spinebbox \u001b[38;5;241m=\u001b[39m \u001b[43mspine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_window_extent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2615\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m   2616\u001b[0m     \u001b[38;5;66;03m# use Axes if spine doesn't exist\u001b[39;00m\n\u001b[1;32m   2617\u001b[0m     spinebbox \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39mbbox\n",
      "File \u001b[0;32m~/mambaforge/envs/PPOenv/lib/python3.12/site-packages/matplotlib/spines.py:158\u001b[0m, in \u001b[0;36mSpine.get_window_extent\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bb\n\u001b[1;32m    157\u001b[0m bboxes \u001b[38;5;241m=\u001b[39m [bb]\n\u001b[0;32m--> 158\u001b[0m drawn_ticks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_ticks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m major_tick \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m({\u001b[38;5;241m*\u001b[39mdrawn_ticks} \u001b[38;5;241m&\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis\u001b[38;5;241m.\u001b[39mmajorTicks}), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    161\u001b[0m minor_tick \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m({\u001b[38;5;241m*\u001b[39mdrawn_ticks} \u001b[38;5;241m&\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis\u001b[38;5;241m.\u001b[39mminorTicks}), \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/mambaforge/envs/PPOenv/lib/python3.12/site-packages/matplotlib/axis.py:1279\u001b[0m, in \u001b[0;36mAxis._update_ticks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1277\u001b[0m major_ticks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_major_ticks(\u001b[38;5;28mlen\u001b[39m(major_locs))\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tick, loc, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(major_ticks, major_locs, major_labels):\n\u001b[0;32m-> 1279\u001b[0m     \u001b[43mtick\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_position\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1280\u001b[0m     tick\u001b[38;5;241m.\u001b[39mlabel1\u001b[38;5;241m.\u001b[39mset_text(label)\n\u001b[1;32m   1281\u001b[0m     tick\u001b[38;5;241m.\u001b[39mlabel2\u001b[38;5;241m.\u001b[39mset_text(label)\n",
      "File \u001b[0;32m~/mambaforge/envs/PPOenv/lib/python3.12/site-packages/matplotlib/axis.py:512\u001b[0m, in \u001b[0;36mYTick.update_position\u001b[0;34m(self, loc)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtick1line\u001b[38;5;241m.\u001b[39mset_ydata((loc,))\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtick2line\u001b[38;5;241m.\u001b[39mset_ydata((loc,))\n\u001b[0;32m--> 512\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgridline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_ydata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel1\u001b[38;5;241m.\u001b[39mset_y(loc)\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel2\u001b[38;5;241m.\u001b[39mset_y(loc)\n",
      "File \u001b[0;32m~/mambaforge/envs/PPOenv/lib/python3.12/site-packages/matplotlib/lines.py:1291\u001b[0m, in \u001b[0;36mLine2D.set_ydata\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m   1288\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invalidx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_ydata\u001b[39m(\u001b[38;5;28mself\u001b[39m, y):\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;124;03m    Set the data array for y.\u001b[39;00m\n\u001b[1;32m   1294\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;124;03m    y : 1D array\u001b[39;00m\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39miterable(y):\n\u001b[1;32m   1300\u001b[0m         \u001b[38;5;66;03m# When deprecation cycle is completed\u001b[39;00m\n\u001b[1;32m   1301\u001b[0m         \u001b[38;5;66;03m# raise RuntimeError('y must be a sequence')\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## HYPERPARAMETERS\n",
    "num_trajectories = 100 # How many trajectories do we want to collect per training\n",
    "T = 500  # Maximum number of steps per trajectory\n",
    "gamma = 0.90  # Discount factor for future rewards\n",
    "lambda_ = 0.95\n",
    "v = 0.5 # weighting factor for value loss\n",
    "h = 0.5 # weighting factor for entropy loss\n",
    "lr = 0.01 # learning rate\n",
    "minibatch_size = 128 # minibatch size\n",
    "clip_epsilon = 0.2\n",
    "num_epochs = 30\n",
    "epoch_mean = 0\n",
    "angle_penalty = 0.5\n",
    "num_cells = 256 # number of cells for each layer\n",
    "\n",
    "\n",
    "## GYM INITIALIZATION\n",
    "# Creation of gym\n",
    "# video mode\n",
    "#env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "action_space_size = env.action_space.n\n",
    "step = torch.tensor(env.reset()[0])\n",
    "\n",
    "## INITIALIZATION OF NETWORKS\n",
    "\n",
    "# define policy network\n",
    "policy_net = DiscretePolicyNetwork(num_cells, action_space_size)\n",
    "dummy_output = policy_net(step)\n",
    "policy_net.apply(init_weights)\n",
    "\n",
    "# define value network\n",
    "value_net = ValueNetwork(num_cells)\n",
    "dummy_output = value_net(step)\n",
    "value_net.apply(init_weights)\n",
    "\n",
    "\n",
    "optimizer_policy = torch.optim.Adam(policy_net.parameters(), lr = lr)\n",
    "optimizer_value = torch.optim.Adam(value_net.parameters(), lr = lr)\n",
    "# Define the learning rate scheduler\n",
    "scheduler_policy = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_policy, 'max', patience=15, factor=0.8, verbose=True)\n",
    "scheduler_value = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_value, 'max', patience=15, factor=0.8, verbose=True)\n",
    "\n",
    "\n",
    "epoch_losses = []  # List to store losses for each epoch\n",
    "average_rewards = []  # List to store the average rewards\n",
    "max_episode_lengths = []\n",
    "median_rewards = []\n",
    "outer_loop = 0\n",
    "plt.ion()\n",
    "\n",
    "while True:\n",
    "    training_data = []\n",
    "    total_rewards = []\n",
    "    episode_lengths = []\n",
    "\n",
    "    for i in range(num_trajectories):\n",
    "        state = torch.tensor(env.reset()[0])\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "\n",
    "        trajectory = collect_trajectory(T, state, policy_net, angle_penalty)\n",
    "        trajectory = extend_trajectory(T, trajectory, value_net, gamma, lambda_)\n",
    "\n",
    "        for step in trajectory:\n",
    "            total_reward += step[2]  # Assuming the reward is the second element in the step\n",
    "            steps += 1\n",
    "\n",
    "        training_data.append(trajectory)\n",
    "        total_rewards.append(total_reward)\n",
    "        episode_lengths.append(steps)\n",
    "\n",
    "    # After collecting all trajectories, calculate the average reward and the maximum episode length\n",
    "    average_reward = sum(total_rewards) / len(total_rewards)\n",
    "    max_episode_length = max(episode_lengths)\n",
    "    average_rewards.append(average_reward)\n",
    "    max_episode_lengths.append(max_episode_length)\n",
    "    median_reward = np.median(total_rewards)\n",
    "    median_rewards.append(median_reward)\n",
    "\n",
    "\n",
    "    # Training loop for each epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        minibatches = create_minibatches(training_data, minibatch_size)\n",
    "        epoch_loss = []  # List to store losses for each minibatch in the current epoch\n",
    "\n",
    "        for minibatch in minibatches:\n",
    "        \n",
    "          # Zero the gradients (PyTorch accumulates gradients by default)\n",
    "          optimizer_policy.zero_grad()\n",
    "          optimizer_value.zero_grad()\n",
    "\n",
    "          # Turn on gradient tracking\n",
    "          policy_net.train()\n",
    "          value_net.train()\n",
    "\n",
    "          L_v, L_clipvh = calc_lclipvh(minibatch, policy_net, value_net, v, h, clip_epsilon)\n",
    "        \n",
    "          L_clipvh.backward()\n",
    "          L_v\n",
    "          print(f\"Gradients of policy network parameters: {[param.grad for param in policy_net.parameters()]}\")\n",
    "          print(f\"Gradients should not be None and should have variability\")\n",
    "\n",
    "          optimizer_policy.step()\n",
    "          optimizer_value.step()\n",
    "          epoch_loss.append(L_clipvh.item())\n",
    "\n",
    "        #scheduler_policy.step(average_reward)\n",
    "        #scheduler_value.step(average_reward)\n",
    "\n",
    "        epoch_mean = sum(epoch_loss) / len(epoch_loss)\n",
    "        epoch_losses.append(epoch_mean)\n",
    "\n",
    "\n",
    "    # Update the plot\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(10, 14))\n",
    "    plt.subplot(4,2,1)\n",
    "    plt.plot(average_rewards, label='Average Reward', color = \"blue\")\n",
    "    plt.plot(median_rewards, label=\"Median Reward\", color = \"green\")\n",
    "    plt.xlabel('Loop')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title('Average and Median Reward Over Time')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(4,2,2)\n",
    "    plt.plot(max_episode_lengths, label='Maximum Length', color ='red')\n",
    "    plt.xlabel('Loop')\n",
    "    plt.ylabel('Maximum Length')\n",
    "    plt.title('Maximum Length Over Time')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(4,2,3)\n",
    "    plt.hist(total_rewards, bins = 30, label='Reward Distribution')\n",
    "    plt.xlabel('Reward')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Reward Distribution Over Time')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(4,2,4)\n",
    "    plt.plot(epoch_losses, label='Epoch Loss', color = \"purple\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Epoch Loss Over Time')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    outer_loop += 1\n",
    "\n",
    "# Turn off interactive plotting for final display\n",
    "plt.ioff()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PPOenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
